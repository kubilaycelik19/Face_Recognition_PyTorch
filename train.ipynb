{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import dlib\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindataset = datasets.ImageFolder(root=\"images/\", transform=transform)\n",
    "testdataset = datasets.ImageFolder(root=\"test/\", transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(traindataset, batch_size=8, shuffle=True)\n",
    "test_dataloader = DataLoader(testdataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images batch shape: torch.Size([8, 3, 256, 256])\n",
      "Labels: tensor([1, 1, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Verileri incele\n",
    "for images, labels in train_dataloader:\n",
    "    print(f\"Images batch shape: {images.size()}\")  # Her batch'in boyutunu göster\n",
    "    print(f\"Labels: {labels}\")  # Her batch'in etiketlerini göster (0 = kubilay, 1 = furkan)\n",
    "    break  # İlk batch'i gösterip döngüden çık"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "İlk görüntünün boyutu: torch.Size([3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# train_dataloader'dan bir batch alın\n",
    "images, labels = next(iter(train_dataloader))\n",
    "\n",
    "# İlk görüntünün boyutunu yazdırın\n",
    "print(f\"İlk görüntünün boyutu: {images[7].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['furkan', 'kubilay']\n",
      "{'furkan': 0, 'kubilay': 1}\n"
     ]
    }
   ],
   "source": [
    "# Sınıfları görüntüle\n",
    "print(traindataset.classes)  # ['furkan', 'kubilay']\n",
    "print(traindataset.class_to_idx)  # {'furkan': 0, 'kubilay': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN tabanlı FaceNet benzeri model\n",
    "faceNetModel = nn.Sequential(\n",
    "    # 1. Conv Bloğu\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # Giriş: 3, Çıkış: 64\n",
    "    nn.BatchNorm2d(64),  # Burada çıkış kanalları 64 olmalı\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Boyutları yarıya indirir\n",
    "    \n",
    "    # 2. Conv Bloğu\n",
    "    nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # Giriş: 64, Çıkış: 128\n",
    "    nn.BatchNorm2d(128),  # Burada çıkış kanalları 128 olmalı\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Boyutları yarıya indirir\n",
    "\n",
    "    # 3. Conv Bloğu\n",
    "    nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),  # Giriş: 128, Çıkış: 256\n",
    "    nn.BatchNorm2d(256),  # Burada çıkış kanalları 256 olmalı\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),  # Boyutları yarıya indirir\n",
    "\n",
    "    # Flatten ve Fully Connected katmanlar\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(256 * 32 * 32, 256),  # Girdi boyutu: 256 x 32 x 32, toplam 256 * 32 * 32 özellik\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),  # Overfitting'i önlemek için dropout\n",
    "    \n",
    "    nn.Linear(256, 128),  # 128 boyutlu embedding\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 2)  # 2 sınıf: kubilay ve furkan\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kayıp fonksiyonu ve optimizasyon\n",
    "criterion = nn.CrossEntropyLoss()  # Çok sınıflı sınıflandırma kayıp fonksiyonu\n",
    "optimizer = optim.Adam(faceNetModel.parameters(), lr=0.001)  # Adam optimizasyon algoritması\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "faceNetModel = faceNetModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim fonksiyonu\n",
    "def train(model, dataloader, criterion, optimizer, epochs=10):\n",
    "    model.train()  # Modeli eğitim moduna al\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for images, labels in dataloader:\n",
    "            # Verileri GPU'ya taşı\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Sıfırla gradientler\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # İleri geçiş\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Kayıp hesapla\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Geri geçiş ve optimizasyon\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Kayıp değerini biriktir\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Her epoch sonunda kayıp değerini göster\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.9733\n",
      "Epoch [2/10], Loss: 0.8008\n",
      "Epoch [3/10], Loss: 0.0000\n",
      "Epoch [4/10], Loss: 0.0000\n",
      "Epoch [5/10], Loss: 0.0000\n",
      "Epoch [6/10], Loss: 0.2179\n",
      "Epoch [7/10], Loss: 0.8957\n",
      "Epoch [8/10], Loss: 0.1373\n",
      "Epoch [9/10], Loss: 3.6393\n",
      "Epoch [10/10], Loss: 0.1698\n"
     ]
    }
   ],
   "source": [
    "train(faceNetModel, train_dataloader, criterion, optimizer, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(faceNetModel, \"FirstTrainFaceNet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\celik\\AppData\\Local\\Temp\\ipykernel_9708\\2002457176.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  loadedModel = torch.load(\"FirstTrainFaceNet.pth\")\n"
     ]
    }
   ],
   "source": [
    "loadedModel = torch.load(\"FirstTrainFaceNet.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU()\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): ReLU()\n",
       "  (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (10): ReLU()\n",
       "  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (12): Flatten(start_dim=1, end_dim=-1)\n",
       "  (13): Linear(in_features=262144, out_features=256, bias=True)\n",
       "  (14): ReLU()\n",
       "  (15): Dropout(p=0.5, inplace=False)\n",
       "  (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (17): ReLU()\n",
       "  (18): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadedModel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    model.eval()  # Modeli değerlendirme moduna al\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Gradyan hesaplamayı kapat\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Verileri GPU'ya taşı\n",
    "            \n",
    "            # Modelin tahminlerini al\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            # Doğru tahminleri say\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Tahmin ve etiketleri listeye ekle\n",
    "            all_preds.extend(predicted.cpu().numpy())  # Tahminleri CPU'ya taşı ve listeye ekle\n",
    "            all_labels.extend(labels.cpu().numpy())  # Gerçek etiketleri CPU'ya taşı ve listeye ekle\n",
    "\n",
    "    # Doğruluk oranını hesapla\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # F1 skorunu hesapla (weighted average)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 27.27%\n",
      "F1 Score: 0.2143\n"
     ]
    }
   ],
   "source": [
    "test(loadedModel, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
